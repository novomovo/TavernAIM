<html>
    <head>
        <title>KoboldAI Settings</title>
        <link rel="stylesheet" href="/css/notes.css">
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    </head>
    <body>
        <div id="main">
            <div id="content">
                <h2>KoboldAI Settings</h2>
                <p>Standard KoboldAI settings files are used here. To add your own settings, simply add the file .settings in TavernAI\public\KoboldAI Settings</p>
                <h3>Temperature</h3>
                <p>Value from 0.1 to 2.0. Lower value - the answers are more logical, but less creative. Higher value - the answers are more creative, but less logical.</p>
                <h3>Repetition Penalty</h3>
                <p>Used to penalize words that were already generated or belong to the context. The standard value for chat is 1.06</p>
                <h3>Repetition Penalty Range</h3>
                <p>If set higher than 0, only applies repetition penalty to the last few tokens of the context rather than applying it to the entire context. The slider controls the number of tokens at the end of your context to apply it to.</p>
                <h3>Repetition Penalty Slope</h3>
                <p>If both this setting and Repetition Penalty Range are set higher than 0, will use sigmoid interpolation to apply repetition penalty more strongly on tokens that are closer to the end of the context. Higher values will result in the repetition penalty difference between the start and end of your context being more apparent. (Setting this to 1 or less uses linear interpolation; setting this to 0 disables interpolation.)</p>
                <h3>Top-P Sampling</h3>
                <p>1 is disabled</p>
                <p>Top-P is a widely used text generation method that involves converting logits into probabilities using the softmax function. The technique keeps as many tokens as possible while adhering to two rules, which are based on the top-p value. A high top-p value is recommended for better creativity, as lower values limit the number of tokens kept. Setting the top-p value to 0 is equivalent to greedy search.</p>
                <h3>Top-K Sampling</h3>
                <p>0 is disabled</p>
                <p>Top-K leaves the largest k logits unchanged while setting all the others to negative infinity. However, it has been found to be less effective than other sampling techniques and is often used as a permissive filter before implementing more advanced methods. It is recommended to use top-k sampling as the first sampler in the model to avoid nullifying the effects of more intelligent samplers.</p>
                <h3>Top-A Sampling</h3>
                <p>0 is disabled</p>
                <p>Top-A sampling is a relatively new sampling method designed for use with BlinkDL's RWKV language models. It involves converting logits into probabilities using the softmax function and setting the logits of tokens with a probability less than a certain value (the top-a value) to negative infinity. One of the highest probability tokens must always be kept, even if its probability is less than the top-a value. Top-a sampling reduces randomness when the model is confident about the next token, but has little effect on creativity.</p>
                <h3>Typical Sampling</h3>
                <p>1 is disabled</p>
                <p>Typical Sampling aims to keep the information content of text consistent throughout generated text. It works by sorting tokens in ascending order of their absolute value of entropy and natural logarithm of probability, and keeping the minimum possible number of tokens that exceed a certain probability threshold. This method can strongly affect the content of the output but still maintains creativity even at extremely low settings.</p>
                <h3>Tail Free Sampling</h3>
                <p>1 is disabled</p>
                <p>Tail Free Sampling aims to remove low probability tokens without compromising the creativity of the generated text. It does this by identifying a "tail" of undesirable tokens in the probability distribution and removing them based on a user-specified threshold. This method is designed to work well on longer pieces of text and can be used in conjunction with other sampling methods for further control over the generated output.</p>
                <h3>Amount Generation</h3>
                <p>The maximum number of tokens that a AI will generate when responding to the prompt. One word is approximately 3-4 tokens. The larger the parameter value, the longer the generation time takes.</p>
                <h3>Context Size</h3>
                <p>How much will the AI remember. Context size also affects the speed of generation.</p>
                <br>
                <p><u>Important</u>: The setting of Context Size in TavernAI GUI always overrides the setting in KoboldAI GUI!</p>
            </div>
        </div>
    </body>
</html>
